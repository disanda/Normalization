# 网络权重初始化

y = wx

网络的每一层类似矩阵运算，
如果网络权重w不初始化，
在深层网络时，容易发生梯度消失或梯度爆炸

默认用uniform:[-1,1]或者normal:[mean=0,std=1]初始化
但是如果不加而外的衰减，高层训练会发生衰减


# Xavier

最早来自2010年的论文:《Understanding the difficulty of training deep feedforward neural networks》，在2016年开始流行

只是对对称的激活函数且mean=0，std=1时有用

```py
def xavier(size):
    in_dim = size[0]
    xavier_stddev = 1. / np.sqrt(in_dim / 2.)
    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)
    
def xavier(m,n):
    return torch.Tensor(m,n).uniform_(-1,1)*math.sqrt(6./(m+h))
```


# Kaiming

这个可以解决不对称的情况，即使用relu的情况

```
def kaiming(m,n):
    return torch.randn(m,h)*math.sqrt(2./m)
```

# reference

- https://zhuanlan.zhihu.com/p/62850258
- https://zhuanlan.zhihu.com/p/22028079
